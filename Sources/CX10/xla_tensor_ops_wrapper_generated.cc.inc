// Autogenerated by codegen.py. Do not modify.

namespace swift_xla {
namespace ir {
namespace ops {
namespace {

class Abs : public Node {
 public:
  Abs(const Value& input)
      : Node(ir::OpKind(at::aten::abs),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Abs>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildAbs(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Acos : public Node {
 public:
  Acos(const Value& input)
      : Node(ir::OpKind(at::aten::acos),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Acos>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Acos(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Acosh : public Node {
 public:
  Acosh(const Value& input)
      : Node(ir::OpKind(at::aten::acosh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Acosh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Acosh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Add : public Node {
 public:
  Add(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::add),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Add>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Add>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Add>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class All : public Node {
 public:
  All(const Value& input, std::vector<xla::int64> dims, bool keep_reduced_dimensions)
      : Node(ir::OpKind(at::aten::all),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = BuildAll(
         input_ir, dims, keep_reduced_dimensions);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dims, keep_reduced_dimensions)),
        dims_(std::move(dims)),
        keep_reduced_dimensions_(std::move(keep_reduced_dimensions)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<All>(
        operands.at(0), dims_, keep_reduced_dimensions_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildAll(
        loctx->GetOutputOp(operand(0)), dims_, keep_reduced_dimensions_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    OpFieldToString(ss, "keep_reduced_dimensions", keep_reduced_dimensions_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
  bool keep_reduced_dimensions_;
};

class Any : public Node {
 public:
  Any(const Value& input, std::vector<xla::int64> dims, bool keep_reduced_dimensions)
      : Node(ir::OpKind(at::aten::any),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = BuildAny(
         input_ir, dims, keep_reduced_dimensions);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dims, keep_reduced_dimensions)),
        dims_(std::move(dims)),
        keep_reduced_dimensions_(std::move(keep_reduced_dimensions)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Any>(
        operands.at(0), dims_, keep_reduced_dimensions_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildAny(
        loctx->GetOutputOp(operand(0)), dims_, keep_reduced_dimensions_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    OpFieldToString(ss, "keep_reduced_dimensions", keep_reduced_dimensions_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
  bool keep_reduced_dimensions_;
};

class Argmax : public Node {
 public:
  Argmax(const Value& input, xla::int64 dim, bool keepdim)
      : Node(ir::OpKind(at::aten::argmax),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = BuildArgMax(
         input_ir, dim, keepdim);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dim, keepdim)),
        dim_(std::move(dim)),
        keepdim_(std::move(keepdim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Argmax>(
        operands.at(0), dim_, keepdim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildArgMax(
        loctx->GetOutputOp(operand(0)), dim_, keepdim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "keepdim", keepdim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool keepdim_;
};

class Argmin : public Node {
 public:
  Argmin(const Value& input, xla::int64 dim, bool keepdim)
      : Node(ir::OpKind(at::aten::argmin),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = BuildArgMin(
         input_ir, dim, keepdim);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dim, keepdim)),
        dim_(std::move(dim)),
        keepdim_(std::move(keepdim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Argmin>(
        operands.at(0), dim_, keepdim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildArgMin(
        loctx->GetOutputOp(operand(0)), dim_, keepdim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "keepdim", keepdim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool keepdim_;
};

class Asin : public Node {
 public:
  Asin(const Value& input)
      : Node(ir::OpKind(at::aten::asin),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Asin>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Asin(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Asinh : public Node {
 public:
  Asinh(const Value& input)
      : Node(ir::OpKind(at::aten::asinh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Asinh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Asinh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Atan : public Node {
 public:
  Atan(const Value& input)
      : Node(ir::OpKind(at::aten::atan),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Atan>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Atan(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Atanh : public Node {
 public:
  Atanh(const Value& input)
      : Node(ir::OpKind(at::aten::atanh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Atanh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Atanh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Cat : public Node {
 public:
  Cat(absl::Span<const Value> input, xla::int64 dim)
      : Node(
            ir::OpKind(at::aten::cat), input,
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = MakeParameterList(&b, 0, input, "p0");
              xla::XlaOp result = BuildCat(input_ir, dim);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cat>(operands.subspan(0), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildCat(GetArrayOperands(loctx, operands(), 0), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class Ceil : public Node {
 public:
  Ceil(const Value& input)
      : Node(ir::OpKind(at::aten::ceil),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Ceil>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Ceil(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Clamp : public Node {
 public:
  Clamp(const Value& t, const Value& clipValueMin, const Value& clipValueMax)
      : Node(ir::OpKind(at::aten::clamp), {t, clipValueMin, clipValueMax},
             t.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Clamp>(
        operands.at(0), operands.at(1), operands.at(2));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerClamp(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), loctx->GetOutputOp(operand(2)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class ConstantPadNd : public Node {
 public:
  ConstantPadNd(const Value& input, std::vector<xla::int64> pad, at::Scalar value)
      : Node(ir::OpKind(at::aten::constant_pad_nd),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = LowerPad(
         input_ir, pad, value);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(pad, value)),
        pad_(std::move(pad)),
        value_(std::move(value)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<ConstantPadNd>(
        operands.at(0), pad_, value_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerPad(
        loctx->GetOutputOp(operand(0)), pad_, value_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "pad", pad_);
    OpFieldToString(ss, "value", value_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> pad_;
  at::Scalar value_;};

class Cos : public Node {
 public:
  Cos(const Value& input)
      : Node(ir::OpKind(at::aten::cos),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cos>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Cos(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Cosh : public Node {
 public:
  Cosh(const Value& input)
      : Node(ir::OpKind(at::aten::cosh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cosh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Cosh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Cumprod : public Node {
 public:
  Cumprod(const Value& input, xla::int64 dim, bool exclusive, bool reverse)
      : Node(ir::OpKind(at::aten::cumprod), {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim, exclusive, reverse)),
        dim_(std::move(dim)),
        exclusive_(std::move(exclusive)),
        reverse_(std::move(reverse)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cumprod>(operands.at(0), dim_, exclusive_, reverse_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerCumProd(loctx->GetOutputOp(operand(0)), dim_,
                                     exclusive_, reverse_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "exclusive", exclusive_);
    OpFieldToString(ss, "reverse", reverse_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool exclusive_;
  bool reverse_;
};

class Cumsum : public Node {
 public:
  Cumsum(const Value& input, xla::int64 dim, bool exclusive, bool reverse)
      : Node(ir::OpKind(at::aten::cumsum), {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim, exclusive, reverse)),
        dim_(std::move(dim)),
        exclusive_(std::move(exclusive)),
        reverse_(std::move(reverse)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Cumsum>(operands.at(0), dim_, exclusive_, reverse_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        LowerCumSum(loctx->GetOutputOp(operand(0)), dim_, exclusive_, reverse_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "exclusive", exclusive_);
    OpFieldToString(ss, "reverse", reverse_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool exclusive_;
  bool reverse_;
};

class DiagonalValue : public Node {
 public:
  DiagonalValue(const Value& input, xla::int64 offset, xla::int64 dim1,
                xla::int64 dim2)
      : Node(
            ir::OpKind(at::aten::diagonal), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildDiagonal(input_ir, offset, dim1, dim2);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(offset, dim1, dim2)),
        offset_(std::move(offset)),
        dim1_(std::move(dim1)),
        dim2_(std::move(dim2)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<DiagonalValue>(operands.at(0), offset_, dim1_, dim2_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildDiagonal(loctx->GetOutputOp(operand(0)), offset_, dim1_, dim2_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "offset", offset_);
    OpFieldToString(ss, "dim1", dim1_);
    OpFieldToString(ss, "dim2", dim2_);
    return ss.str();
  }

 private:
  xla::int64 offset_;
  xla::int64 dim1_;
  xla::int64 dim2_;
};

class Div : public Node {
 public:
  Div(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::div),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Div>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Div>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Div>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class DynamicSlice : public Node {
 public:
  DynamicSlice(const Value& base, absl::Span<const Value> start_indices,
               std::vector<xla::int64> slice_shapes)
      : Node(
            ir::OpKind(at::aten::xla_dynamic_slice),
            TensorArgsConcat({base}, start_indices),
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto base_ir = xla::Parameter(&b, 0, base.shape(), "p0");
              auto start_indices_ir =
                  MakeParameterList(&b, 1, start_indices, "p1");
              xla::XlaOp result =
                  xla::DynamicSlice(base_ir, start_indices_ir, slice_shapes);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(slice_shapes)),
        slice_shapes_(std::move(slice_shapes)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<DynamicSlice>(operands.at(0), operands.subspan(1),
                                  slice_shapes_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::DynamicSlice(
        loctx->GetOutputOp(operand(0)), GetArrayOperands(loctx, operands(), 1),
        slice_shapes_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "slice_shapes", slice_shapes_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> slice_shapes_;
};

class DynamicUpdateSlice : public Node {
 public:
  DynamicUpdateSlice(const Value& base, const Value& update,
                     absl::Span<const Value> start_indices)
      : Node(
            ir::OpKind(at::aten::xla_dynamic_update_slice),
            TensorArgsConcat({base, update}, start_indices),
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto base_ir = xla::Parameter(&b, 0, base.shape(), "p0");
              auto update_ir = xla::Parameter(&b, 1, update.shape(), "p1");
              auto start_indices_ir =
                  MakeParameterList(&b, 2, start_indices, "p2");
              xla::XlaOp result =
                  xla::DynamicUpdateSlice(base_ir, update_ir, start_indices_ir);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<DynamicUpdateSlice>(operands.at(0), operands.at(1),
                                        operands.subspan(2));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::DynamicUpdateSlice(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)),
        GetArrayOperands(loctx, operands(), 2));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Eq : public Node {
 public:
  Eq(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::eq),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Eq>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Eq>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Eq>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Exp : public Node {
 public:
  Exp(const Value& input)
      : Node(ir::OpKind(at::aten::exp),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Exp>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Exp(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Expand : public Node {
 public:
  Expand(const Value& input, std::vector<xla::int64> dims)
      : Node(ir::OpKind(at::aten::expand),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = BuildExpand(
         input_ir, dims);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dims)),
        dims_(std::move(dims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Expand>(
        operands.at(0), dims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildExpand(
        loctx->GetOutputOp(operand(0)), dims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
};

class Expm1 : public Node {
 public:
  Expm1(const Value& input)
      : Node(ir::OpKind(at::aten::expm1),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Expm1>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Expm1(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Flip : public Node {
 public:
  Flip(const Value& input, std::vector<xla::int64> dims)
      : Node(ir::OpKind(at::aten::flip),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(dims)),
        dims_(std::move(dims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Flip>(
        operands.at(0), dims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Rev(
        loctx->GetOutputOp(operand(0)), dims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
};

class Floor : public Node {
 public:
  Floor(const Value& input)
      : Node(ir::OpKind(at::aten::floor),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Floor>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Floor(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Gather : public Node {
 public:
  Gather(const Value& input, const Value& indices, xla::int64 start_dim)
      : Node(
            ir::OpKind(at::aten::index), {input, indices},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto indices_ir = xla::Parameter(&b, 1, indices.shape(), "p1");
              xla::XlaOp result = CreateIndex(input_ir, indices_ir, start_dim);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(start_dim)),
        start_dim_(std::move(start_dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Gather>(operands.at(0), operands.at(1), start_dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = CreateIndex(loctx->GetOutputOp(operand(0)),
                                    loctx->GetOutputOp(operand(1)), start_dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "start_dim", start_dim_);
    return ss.str();
  }

 private:
  xla::int64 start_dim_;
};

class Ge : public Node {
 public:
  Ge(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::ge),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Ge>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Ge>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Ge>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Gt : public Node {
 public:
  Gt(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::gt),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Gt>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Gt>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Gt>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class IsFinite : public Node {
 public:
  IsFinite(const Value& input)
      : Node(ir::OpKind(at::aten::xla_is_finite),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<IsFinite>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::IsFinite(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class IsInf : public Node {
 public:
  IsInf(const Value& input)
      : Node(ir::OpKind(at::aten::xla_is_inf),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<IsInf>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::IsInf(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class IsNan : public Node {
 public:
  IsNan(const Value& input)
      : Node(ir::OpKind(at::aten::xla_is_nan),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<IsNan>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::IsNan(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Le : public Node {
 public:
  Le(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::le),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Le>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Le>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Le>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Log : public Node {
 public:
  Log(const Value& input)
      : Node(ir::OpKind(at::aten::log),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Log>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Log(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Log1p : public Node {
 public:
  Log1p(const Value& input)
      : Node(ir::OpKind(at::aten::log1p),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Log1p>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Log1p(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class LogSoftmax : public Node {
 public:
  LogSoftmax(const Value& input, xla::int64 dim)
      : Node(ir::OpKind(at::aten::log_softmax),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogSoftmax>(
        operands.at(0), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildLogSoftmax(
        loctx->GetOutputOp(operand(0)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class LogSoftmaxBackward : public Node {
 public:
  LogSoftmaxBackward(const Value& gradOutput, const Value& output,
                     xla::int64 dim)
      : Node(ir::OpKind(at::aten::_log_softmax_backward_data),
             {gradOutput, output}, gradOutput.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogSoftmaxBackward>(
        operands.at(0), operands.at(1), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildLogSoftmaxGrad(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class LogicalAnd : public Node {
 public:
  LogicalAnd(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::logical_and),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::And>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogicalAnd>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::And>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class LogicalCast : public Node {
 public:
  LogicalCast(const Value& input, at::ScalarType dtype)
      : Node(ir::OpKind(xla_symbols::cast),
             {input}, ShapeLogicalCast(input, dtype),
             /*num_outputs=*/1, xla::util::MHash(dtype)),
        dtype_(std::move(dtype)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogicalCast>(
        operands.at(0), dtype_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerLogicalCast(
        loctx->GetOutputOp(operand(0)), dtype_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dtype", dtype_);
    return ss.str();
  }

 private:
  at::ScalarType dtype_;
};

class LogicalNot : public Node {
 public:
  LogicalNot(const Value& input)
      : Node(ir::OpKind(at::aten::bitwise_not),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogicalNot>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Not(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class LogicalOr : public Node {
 public:
  LogicalOr(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::logical_or),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Or>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<LogicalOr>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Or>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Lt : public Node {
 public:
  Lt(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::lt),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Lt>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Lt>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Lt>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Matmul : public Node {
 public:
  Matmul(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::matmul),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryValueOp<CreateMatMul>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Matmul>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryValueOp<CreateMatMul>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Max : public Node {
 public:
  Max(const Value& input, xla::int64 dim, bool keepDim)
      : Node(
            ir::OpKind(at::aten::max), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildMaxInDim(input_ir, dim, keepDim);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dim, keepDim)),
        dim_(std::move(dim)),
        keepDim_(std::move(keepDim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Max>(operands.at(0), dim_, keepDim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildMaxInDim(loctx->GetOutputOp(operand(0)), dim_, keepDim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "keepDim", keepDim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool keepDim_;
};

class Maximum : public Node {
 public:
  Maximum(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::max),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Max>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Maximum>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Max>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Mean : public Node {
 public:
  Mean(const Value& input, std::vector<xla::int64> reductionIndices,
       bool keepDims)
      : Node(
            ir::OpKind(at::aten::mean), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result =
                  BuildMean(input_ir, reductionIndices, keepDims);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(reductionIndices, keepDims)),
        reductionIndices_(std::move(reductionIndices)),
        keepDims_(std::move(keepDims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Mean>(operands.at(0), reductionIndices_, keepDims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildMean(loctx->GetOutputOp(operand(0)), reductionIndices_, keepDims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "reductionIndices", reductionIndices_);
    OpFieldToString(ss, "keepDims", keepDims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> reductionIndices_;
  bool keepDims_;
};

class Min : public Node {
 public:
  Min(const Value& input, xla::int64 dim, bool keepDim)
      : Node(
            ir::OpKind(at::aten::min), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildMinInDim(input_ir, dim, keepDim);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dim, keepDim)),
        dim_(std::move(dim)),
        keepDim_(std::move(keepDim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Min>(operands.at(0), dim_, keepDim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildMinInDim(loctx->GetOutputOp(operand(0)), dim_, keepDim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "keepDim", keepDim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  bool keepDim_;
};

class Minimum : public Node {
 public:
  Minimum(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::min),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Min>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Minimum>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Min>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Mm : public Node {
 public:
  Mm(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::mm),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = xla::Dot(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Mm>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Dot(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Mul : public Node {
 public:
  Mul(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::mul),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Mul>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Mul>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Mul>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Ne : public Node {
 public:
  Ne(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::ne),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Ne>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Ne>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Ne>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Neg : public Node {
 public:
  Neg(const Value& input)
      : Node(ir::OpKind(at::aten::neg),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Neg>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Neg(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class NllLoss : public Node {
 public:
  NllLoss(const Value& logits, const Value& labels, xla::int64 ignore_index)
      : Node(
            ir::OpKind(at::aten::nll_loss), {logits, labels},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto logits_ir = xla::Parameter(&b, 0, logits.shape(), "p0");
              auto labels_ir = xla::Parameter(&b, 1, labels.shape(), "p1");
              xla::XlaOp result =
                  LowerNllLoss(logits_ir, labels_ir, ignore_index);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(ignore_index)),
        ignore_index_(std::move(ignore_index)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<NllLoss>(operands.at(0), operands.at(1), ignore_index_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        LowerNllLoss(loctx->GetOutputOp(operand(0)),
                     loctx->GetOutputOp(operand(1)), ignore_index_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "ignore_index", ignore_index_);
    return ss.str();
  }

 private:
  xla::int64 ignore_index_;
};

class PermuteValue : public Node {
 public:
  PermuteValue(const Value& input, std::vector<xla::int64> dims)
      : Node(
            ir::OpKind(at::aten::permute), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = xla::Transpose(input_ir, dims);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dims)),
        dims_(std::move(dims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<PermuteValue>(operands.at(0), dims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Transpose(loctx->GetOutputOp(operand(0)), dims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
};

class PhysicalCast : public Node {
 public:
  PhysicalCast(const Value& input, at::ScalarType dtype)
      : Node(ir::OpKind(xla_symbols::cast), {input},
             ShapeLogicalCast(input, dtype),
             /*num_outputs=*/1, xla::util::MHash(dtype)),
        dtype_(std::move(dtype)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<PhysicalCast>(operands.at(0), dtype_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        LowerLogicalCast(loctx->GetOutputOp(operand(0)), dtype_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dtype", dtype_);
    return ss.str();
  }

 private:
  at::ScalarType dtype_;
};

class Pow : public Node {
 public:
  Pow(const Value& input, const Value& other)
      : Node(ir::OpKind(at::aten::pow),
             {input, other}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       auto other_ir = xla::Parameter(&b, 1, other.shape(), "p1");
       xla::XlaOp result = xla::Pow(
         input_ir, other_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Pow>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Pow(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Prod : public Node {
 public:
  Prod(const Value& input, std::vector<xla::int64> reductionIndices,
       bool keepDims)
      : Node(
            ir::OpKind(at::aten::prod), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result =
                  LowerProd(input_ir, reductionIndices, keepDims);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(reductionIndices, keepDims)),
        reductionIndices_(std::move(reductionIndices)),
        keepDims_(std::move(keepDims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Prod>(operands.at(0), reductionIndices_, keepDims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        LowerProd(loctx->GetOutputOp(operand(0)), reductionIndices_, keepDims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "reductionIndices", reductionIndices_);
    OpFieldToString(ss, "keepDims", keepDims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> reductionIndices_;
  bool keepDims_;
};

class Qr : public Node {
 public:
  Qr(const Value& input, bool fullMatrices)
      : Node(
            ir::OpKind(at::aten::qr), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto results = LowerQR(input_ir, fullMatrices);
              return ShapeOfXlaOpList(results);
            },
            /*num_outputs=*/2, xla::util::MHash(fullMatrices)),
        fullMatrices_(std::move(fullMatrices)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Qr>(operands.at(0), fullMatrices_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    auto result = LowerQR(loctx->GetOutputOp(operand(0)), fullMatrices_);
    return ReturnOps(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "fullMatrices", fullMatrices_);
    return ss.str();
  }

 private:
  bool fullMatrices_;
};

class Relu : public Node {
 public:
  Relu(const Value& features)
      : Node(
            ir::OpKind(at::aten::relu), {features},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto features_ir = xla::Parameter(&b, 0, features.shape(), "p0");
              xla::XlaOp result = BuildRelu(features_ir);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Relu>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildRelu(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Rem : public Node {
 public:
  Rem(const Value& input, const Value& other)
      : Node(ir::OpKind(at::aten::xla_rem),
             {input, other}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       auto other_ir = xla::Parameter(&b, 1, other.shape(), "p1");
       xla::XlaOp result = xla::Rem(
         input_ir, other_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Rem>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Rem(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Repeat : public Node {
 public:
  Repeat(const Value& input, std::vector<xla::int64> multiples)
      : Node(
            ir::OpKind(at::aten::repeat), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildRepeat(input_ir, multiples);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(multiples)),
        multiples_(std::move(multiples)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Repeat>(operands.at(0), multiples_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildRepeat(loctx->GetOutputOp(operand(0)), multiples_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "multiples", multiples_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> multiples_;
};

class ResizeValue : public Node {
 public:
  ResizeValue(const Value& input, std::vector<xla::int64> dims)
      : Node(
            ir::OpKind(at::aten::resize), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildResize(input_ir, dims);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dims)),
        dims_(std::move(dims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<ResizeValue>(operands.at(0), dims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildResize(loctx->GetOutputOp(operand(0)), dims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dims", dims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> dims_;
};

class RoundToEven : public Node {
 public:
  RoundToEven(const Value& input)
      : Node(ir::OpKind(at::aten::round_to_even), {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<RoundToEven>(operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::RoundToEven(loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Rsqrt : public Node {
 public:
  Rsqrt(const Value& input)
      : Node(ir::OpKind(at::aten::rsqrt),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Rsqrt>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Rsqrt(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Select : public Node {
 public:
  Select(const Value& input, xla::int64 dim, xla::int64 index)
      : Node(
            ir::OpKind(at::aten::select), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = LowerSelect(input_ir, dim, index);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dim, index)),
        dim_(std::move(dim)),
        index_(std::move(index)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Select>(operands.at(0), dim_, index_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        LowerSelect(loctx->GetOutputOp(operand(0)), dim_, index_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "index", index_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  xla::int64 index_;
};

class Sigmoid : public Node {
 public:
  Sigmoid(const Value& input)
      : Node(ir::OpKind(at::aten::sigmoid),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sigmoid>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildSigmoid(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Sign : public Node {
 public:
  Sign(const Value& input)
      : Node(ir::OpKind(at::aten::sign),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sign>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildSign(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Sin : public Node {
 public:
  Sin(const Value& input)
      : Node(ir::OpKind(at::aten::sin),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sin>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Sin(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Sinh : public Node {
 public:
  Sinh(const Value& input)
      : Node(ir::OpKind(at::aten::sinh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sinh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Sinh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Slice : public Node {
 public:
  Slice(const Value& input, xla::int64 dim, xla::int64 start, xla::int64 end, xla::int64 stride)
      : Node(ir::OpKind(at::aten::slice),
             {input}, ShapeSlice(input, dim, start, end, stride),
             /*num_outputs=*/1, xla::util::MHash(dim, start, end, stride)),
        dim_(std::move(dim)),
        start_(std::move(start)),
        end_(std::move(end)),
        stride_(std::move(stride)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Slice>(
        operands.at(0), dim_, start_, end_, stride_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerSlice(
        loctx->GetOutputOp(operand(0)), dim_, start_, end_, stride_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "start", start_);
    OpFieldToString(ss, "end", end_);
    OpFieldToString(ss, "stride", stride_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
  xla::int64 start_;
  xla::int64 end_;
  xla::int64 stride_;
};

class Softmax : public Node {
 public:
  Softmax(const Value& input, xla::int64 dim)
      : Node(ir::OpKind(at::aten::softmax),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Softmax>(
        operands.at(0), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildSoftmax(
        loctx->GetOutputOp(operand(0)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class Sqrt : public Node {
 public:
  Sqrt(const Value& input)
      : Node(ir::OpKind(at::aten::sqrt),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sqrt>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Sqrt(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Squeeze : public Node {
 public:
  Squeeze(const Value& input, xla::int64 dim)
      : Node(ir::OpKind(at::aten::squeeze),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = LowerSqueeze(
         input_ir, dim);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Squeeze>(
        operands.at(0), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerSqueeze(
        loctx->GetOutputOp(operand(0)), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class Stack : public Node {
 public:
  Stack(absl::Span<const Value> input, xla::int64 dim)
      : Node(
            ir::OpKind(at::aten::stack), input,
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = MakeParameterList(&b, 0, input, "p0");
              xla::XlaOp result = BuildStack(input_ir, dim);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(dim)),
        dim_(std::move(dim)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Stack>(operands.subspan(0), dim_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildStack(GetArrayOperands(loctx, operands(), 0), dim_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "dim", dim_);
    return ss.str();
  }

 private:
  xla::int64 dim_;
};

class Sub : public Node {
 public:
  Sub(const Value& lhs, const Value& rhs)
      : Node(ir::OpKind(at::aten::sub),
             {lhs, rhs}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto lhs_ir = xla::Parameter(&b, 0, lhs.shape(), "p0");
       auto rhs_ir = xla::Parameter(&b, 1, rhs.shape(), "p1");
       xla::XlaOp result = LowerBinaryOp<xla::Sub>(
         lhs_ir, rhs_ir);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sub>(
        operands.at(0), operands.at(1));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerBinaryOp<xla::Sub>(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Sum : public Node {
 public:
  Sum(const Value& input, std::vector<xla::int64> reductionIndices,
      bool keepDims)
      : Node(
            ir::OpKind(at::aten::sum), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result =
                  BuildSum(input_ir, reductionIndices, keepDims);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(reductionIndices, keepDims)),
        reductionIndices_(std::move(reductionIndices)),
        keepDims_(std::move(keepDims)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Sum>(operands.at(0), reductionIndices_, keepDims_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildSum(loctx->GetOutputOp(operand(0)), reductionIndices_, keepDims_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "reductionIndices", reductionIndices_);
    OpFieldToString(ss, "keepDims", keepDims_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> reductionIndices_;
  bool keepDims_;
};

class Svd : public Node {
 public:
  Svd(const Value& input, bool computeUv, bool fullMatrices)
      : Node(
            ir::OpKind(at::aten::svd), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto results = LowerSVD(input_ir, computeUv, fullMatrices);
              return ShapeOfXlaOpList(results);
            },
            /*num_outputs=*/3, xla::util::MHash(computeUv, fullMatrices)),
        computeUv_(std::move(computeUv)),
        fullMatrices_(std::move(fullMatrices)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Svd>(operands.at(0), computeUv_, fullMatrices_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    auto result =
        LowerSVD(loctx->GetOutputOp(operand(0)), computeUv_, fullMatrices_);
    return ReturnOps(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "computeUv", computeUv_);
    OpFieldToString(ss, "fullMatrices", fullMatrices_);
    return ss.str();
  }

 private:
  bool computeUv_;
  bool fullMatrices_;
};

class Tan : public Node {
 public:
  Tan(const Value& input)
      : Node(ir::OpKind(at::aten::tan),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Tan>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Tan(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class Tanh : public Node {
 public:
  Tanh(const Value& input)
      : Node(ir::OpKind(at::aten::tanh),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Tanh>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Tanh(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class TfConv : public Node {
 public:
  TfConv(const Value& input, const Value& filter, bool depthwise,
         std::vector<xla::int64> strides, tensorflow::Padding padding,
         std::vector<xla::int64> explicit_paddings,
         tensorflow::TensorFormat data_format,
         std::vector<xla::int64> dilations)
      : Node(
            ir::OpKind(at::aten::tf_convolution), {input, filter},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto filter_ir = xla::Parameter(&b, 1, filter.shape(), "p1");
              xla::XlaOp result =
                  BuildTfConv(input_ir, filter_ir, depthwise, strides, padding,
                              explicit_paddings, data_format, dilations);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1,
            xla::util::MHash(depthwise, strides, padding, explicit_paddings,
                             data_format, dilations)),
        depthwise_(std::move(depthwise)),
        strides_(std::move(strides)),
        padding_(std::move(padding)),
        explicit_paddings_(std::move(explicit_paddings)),
        data_format_(std::move(data_format)),
        dilations_(std::move(dilations)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfConv>(operands.at(0), operands.at(1), depthwise_,
                            strides_, padding_, explicit_paddings_,
                            data_format_, dilations_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildTfConv(loctx->GetOutputOp(operand(0)),
                    loctx->GetOutputOp(operand(1)), depthwise_, strides_,
                    padding_, explicit_paddings_, data_format_, dilations_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "depthwise", depthwise_);
    OpFieldToString(ss, "strides", strides_);
    OpFieldToString(ss, "padding", padding_);
    OpFieldToString(ss, "explicit_paddings", explicit_paddings_);
    OpFieldToString(ss, "data_format", data_format_);
    OpFieldToString(ss, "dilations", dilations_);
    return ss.str();
  }

 private:
  bool depthwise_;
  std::vector<xla::int64> strides_;
  tensorflow::Padding padding_;
  std::vector<xla::int64> explicit_paddings_;
  tensorflow::TensorFormat data_format_;
  std::vector<xla::int64> dilations_;
};

class TfConvBackpropFilter : public Node {
 public:
  TfConvBackpropFilter(const Value& input, std::vector<xla::int64> filter_sizes,
                       const Value& out_backprop, bool depthwise,
                       std::vector<xla::int64> strides,
                       tensorflow::Padding padding,
                       std::vector<xla::int64> explicit_paddings,
                       tensorflow::TensorFormat data_format,
                       std::vector<xla::int64> dilations)
      : Node(
            ir::OpKind(at::aten::tf_conv_backprop_filter),
            {input, out_backprop},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto out_backprop_ir =
                  xla::Parameter(&b, 1, out_backprop.shape(), "p1");
              xla::XlaOp result = BuildTfConvBackpropFilter(
                  input_ir, filter_sizes, out_backprop_ir, depthwise, strides,
                  padding, explicit_paddings, data_format, dilations);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1,
            xla::util::MHash(filter_sizes, depthwise, strides, padding,
                             explicit_paddings, data_format, dilations)),
        filter_sizes_(std::move(filter_sizes)),
        depthwise_(std::move(depthwise)),
        strides_(std::move(strides)),
        padding_(std::move(padding)),
        explicit_paddings_(std::move(explicit_paddings)),
        data_format_(std::move(data_format)),
        dilations_(std::move(dilations)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfConvBackpropFilter>(
        operands.at(0), filter_sizes_, operands.at(1), depthwise_, strides_,
        padding_, explicit_paddings_, data_format_, dilations_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildTfConvBackpropFilter(
        loctx->GetOutputOp(operand(0)), filter_sizes_,
        loctx->GetOutputOp(operand(1)), depthwise_, strides_, padding_,
        explicit_paddings_, data_format_, dilations_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "filter_sizes", filter_sizes_);
    OpFieldToString(ss, "depthwise", depthwise_);
    OpFieldToString(ss, "strides", strides_);
    OpFieldToString(ss, "padding", padding_);
    OpFieldToString(ss, "explicit_paddings", explicit_paddings_);
    OpFieldToString(ss, "data_format", data_format_);
    OpFieldToString(ss, "dilations", dilations_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> filter_sizes_;
  bool depthwise_;
  std::vector<xla::int64> strides_;
  tensorflow::Padding padding_;
  std::vector<xla::int64> explicit_paddings_;
  tensorflow::TensorFormat data_format_;
  std::vector<xla::int64> dilations_;
};

class TfConvBackpropInput : public Node {
 public:
  TfConvBackpropInput(std::vector<xla::int64> input_sizes, const Value& filter,
                      const Value& out_backprop, bool depthwise,
                      std::vector<xla::int64> strides,
                      tensorflow::Padding padding,
                      std::vector<xla::int64> explicit_paddings,
                      tensorflow::TensorFormat data_format,
                      std::vector<xla::int64> dilations)
      : Node(
            ir::OpKind(at::aten::tf_conv_backprop_input),
            {filter, out_backprop},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto filter_ir = xla::Parameter(&b, 0, filter.shape(), "p0");
              auto out_backprop_ir =
                  xla::Parameter(&b, 1, out_backprop.shape(), "p1");
              xla::XlaOp result = BuildTfConvBackpropInput(
                  input_sizes, filter_ir, out_backprop_ir, depthwise, strides,
                  padding, explicit_paddings, data_format, dilations);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1,
            xla::util::MHash(input_sizes, depthwise, strides, padding,
                             explicit_paddings, data_format, dilations)),
        input_sizes_(std::move(input_sizes)),
        depthwise_(std::move(depthwise)),
        strides_(std::move(strides)),
        padding_(std::move(padding)),
        explicit_paddings_(std::move(explicit_paddings)),
        data_format_(std::move(data_format)),
        dilations_(std::move(dilations)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfConvBackpropInput>(
        input_sizes_, operands.at(0), operands.at(1), depthwise_, strides_,
        padding_, explicit_paddings_, data_format_, dilations_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildTfConvBackpropInput(
        input_sizes_, loctx->GetOutputOp(operand(0)),
        loctx->GetOutputOp(operand(1)), depthwise_, strides_, padding_,
        explicit_paddings_, data_format_, dilations_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "input_sizes", input_sizes_);
    OpFieldToString(ss, "depthwise", depthwise_);
    OpFieldToString(ss, "strides", strides_);
    OpFieldToString(ss, "padding", padding_);
    OpFieldToString(ss, "explicit_paddings", explicit_paddings_);
    OpFieldToString(ss, "data_format", data_format_);
    OpFieldToString(ss, "dilations", dilations_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> input_sizes_;
  bool depthwise_;
  std::vector<xla::int64> strides_;
  tensorflow::Padding padding_;
  std::vector<xla::int64> explicit_paddings_;
  tensorflow::TensorFormat data_format_;
  std::vector<xla::int64> dilations_;
};

class TfMirrorPad : public Node {
 public:
  TfMirrorPad(const Value& input, std::vector<xla::int64> padding,
              tensorflow::MirrorPadMode mode)
      : Node(
            ir::OpKind(at::aten::tf_mirror_pad), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result = BuildMirrorPad(input_ir, padding, mode);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(padding, mode)),
        padding_(std::move(padding)),
        mode_(std::move(mode)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfMirrorPad>(operands.at(0), padding_, mode_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildMirrorPad(loctx->GetOutputOp(operand(0)), padding_, mode_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "padding", padding_);
    OpFieldToString(ss, "mode", mode_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> padding_;
  tensorflow::MirrorPadMode mode_;
};

class TfMirrorPadGrad : public Node {
 public:
  TfMirrorPadGrad(const Value& grad_output, std::vector<xla::int64> input_size,
                  std::vector<xla::int64> padding,
                  tensorflow::MirrorPadMode mode)
      : Node(
            ir::OpKind(at::aten::tf_mirror_pad_backward), {grad_output},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto grad_output_ir =
                  xla::Parameter(&b, 0, grad_output.shape(), "p0");
              xla::XlaOp result = BuildMirrorPadBackward(
                  grad_output_ir, input_size, padding, mode);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(input_size, padding, mode)),
        input_size_(std::move(input_size)),
        padding_(std::move(padding)),
        mode_(std::move(mode)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfMirrorPadGrad>(operands.at(0), input_size_, padding_,
                                     mode_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildMirrorPadBackward(loctx->GetOutputOp(operand(0)),
                                               input_size_, padding_, mode_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "input_size", input_size_);
    OpFieldToString(ss, "padding", padding_);
    OpFieldToString(ss, "mode", mode_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> input_size_;
  std::vector<xla::int64> padding_;
  tensorflow::MirrorPadMode mode_;
};

class TfOneHot : public Node {
 public:
  TfOneHot(const Value& indices, const Value& on_value, const Value& off_value, xla::int64 depth, xla::int64 axis)
      : Node(ir::OpKind(at::aten::tf_one_hot),
             {indices, on_value, off_value}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto indices_ir = xla::Parameter(&b, 0, indices.shape(), "p0");
       auto on_value_ir = xla::Parameter(&b, 1, on_value.shape(), "p1");
       auto off_value_ir = xla::Parameter(&b, 2, off_value.shape(), "p2");
       xla::XlaOp result = BuildOneHot(
         indices_ir, on_value_ir, off_value_ir, depth, axis);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(depth, axis)),
        depth_(std::move(depth)),
        axis_(std::move(axis)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfOneHot>(
        operands.at(0), operands.at(1), operands.at(2), depth_, axis_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildOneHot(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), loctx->GetOutputOp(operand(2)), depth_, axis_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "depth", depth_);
    OpFieldToString(ss, "axis", axis_);
    return ss.str();
  }

 private:
  xla::int64 depth_;
  xla::int64 axis_;
};

class TfStatelessRandomUniform : public Node {
 public:
  TfStatelessRandomUniform(xla::Shape shape, const Value& seeds,
                           const Value& minvalue, const Value& maxvalue)
      : Node(
            ir::OpKind(at::aten::tf_stateless_random_uniform),
            {seeds, minvalue, maxvalue},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto seeds_ir = xla::Parameter(&b, 0, seeds.shape(), "p0");
              auto minvalue_ir = xla::Parameter(&b, 1, minvalue.shape(), "p1");
              auto maxvalue_ir = xla::Parameter(&b, 2, maxvalue.shape(), "p2");
              xla::XlaOp result = LowerTfStatelessRandomUniform(
                  shape, seeds_ir, minvalue_ir, maxvalue_ir);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(shape)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfStatelessRandomUniform>(
        shape(), operands.at(0), operands.at(1), operands.at(2));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerTfStatelessRandomUniform(
        shape(), loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), loctx->GetOutputOp(operand(2)), loctx);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class TfUnsortedSegmentSum : public Node {
 public:
  TfUnsortedSegmentSum(const Value& data, const Value& indicies, xla::int64 num_segments)
      : Node(ir::OpKind(at::aten::tf_unsorted_segment_sum),
             {data, indicies}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto data_ir = xla::Parameter(&b, 0, data.shape(), "p0");
       auto indicies_ir = xla::Parameter(&b, 1, indicies.shape(), "p1");
       xla::XlaOp result = LowerTfUnsortedSegmentSum(
         data_ir, indicies_ir, num_segments);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(num_segments)),
        num_segments_(std::move(num_segments)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TfUnsortedSegmentSum>(
        operands.at(0), operands.at(1), num_segments_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerTfUnsortedSegmentSum(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), num_segments_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "num_segments", num_segments_);
    return ss.str();
  }

 private:
  xla::int64 num_segments_;
};

class Threshold : public Node {
 public:
  Threshold(const Value& input, const Value& output, float threshold, float value)
      : Node(ir::OpKind(at::aten::threshold_backward),
             {input, output}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash(threshold, value)),
        threshold_(std::move(threshold)),
        value_(std::move(value)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Threshold>(
        operands.at(0), operands.at(1), threshold_, value_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = BuildThreshold(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), threshold_, value_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "threshold", threshold_);
    OpFieldToString(ss, "value", value_);
    return ss.str();
  }

 private:
  float threshold_;
  float value_;
};

class Topk : public Node {
 public:
  Topk(const Value& input, xla::int64 k, xla::int64 dim, bool largest)
      : Node(
            ir::OpKind(at::aten::topk), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto results = BuildTopK(input_ir, k, dim, largest);
              return ShapeOfXlaOpList(results);
            },
            /*num_outputs=*/2, xla::util::MHash(k, dim, largest)),
        k_(std::move(k)),
        dim_(std::move(dim)),
        largest_(std::move(largest)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Topk>(operands.at(0), k_, dim_, largest_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    auto result = BuildTopK(loctx->GetOutputOp(operand(0)), k_, dim_, largest_);
    return ReturnOps(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "k", k_);
    OpFieldToString(ss, "dim", dim_);
    OpFieldToString(ss, "largest", largest_);
    return ss.str();
  }

 private:
  xla::int64 k_;
  xla::int64 dim_;
  bool largest_;
};

class TruncatedNormal : public Node {
 public:
  TruncatedNormal(const Value& input)
      : Node(ir::OpKind(at::aten::xla_truncated_normal),
             {input}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<TruncatedNormal>(
        operands.at(0));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = tensorflow::TruncatedNormal(
        loctx->GetOutputOp(operand(0)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class UpdateSlice : public Node {
 public:
  UpdateSlice(const Value& input, const Value& source,
              std::vector<xla::int64> baseIndices)
      : Node(
            ir::OpKind(xla_symbols::update_slice), {input, source},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              auto source_ir = xla::Parameter(&b, 1, source.shape(), "p1");
              xla::XlaOp result =
                  BuildUpdateSlice(input_ir, source_ir, baseIndices);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(baseIndices)),
        baseIndices_(std::move(baseIndices)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<UpdateSlice>(operands.at(0), operands.at(1), baseIndices_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result =
        BuildUpdateSlice(loctx->GetOutputOp(operand(0)),
                         loctx->GetOutputOp(operand(1)), baseIndices_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "baseIndices", baseIndices_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> baseIndices_;
};

class Where : public Node {
 public:
  Where(const Value& condition, const Value& input, const Value& other)
      : Node(ir::OpKind(at::aten::where),
             {condition, input, other}, input.shape(),
             /*num_outputs=*/1, xla::util::MHash()) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<Where>(
        operands.at(0), operands.at(1), operands.at(2));
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerWhere(
        loctx->GetOutputOp(operand(0)), loctx->GetOutputOp(operand(1)), loctx->GetOutputOp(operand(2)));
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    return ss.str();
  }

 private:
};

class XlaPad : public Node {
 public:
  XlaPad(const Value& input, at::Scalar padding_value,
         xla::PaddingConfig padding_config)
      : Node(
            ir::OpKind(at::aten::xla_pad), {input},
            [&]() {
              xla::XlaBuilder b("InferOutputShape");
              auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
              xla::XlaOp result =
                  LowerPad(input_ir, padding_value, padding_config);
              return XlaHelpers::ShapeOfXlaOp(result);
            },
            /*num_outputs=*/1, xla::util::MHash(padding_value, padding_config)),
        padding_value_(std::move(padding_value)),
        padding_config_(std::move(padding_config)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<XlaPad>(operands.at(0), padding_value_, padding_config_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = LowerPad(loctx->GetOutputOp(operand(0)), padding_value_,
                                 padding_config_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "padding_value", padding_value_);
    OpFieldToString(ss, "padding_config", padding_config_);
    return ss.str();
  }

 private:
  at::Scalar padding_value_;
  xla::PaddingConfig padding_config_;
};

class XlaSlice : public Node {
 public:
  XlaSlice(const Value& input, std::vector<xla::int64> start_indices, std::vector<xla::int64> limit_indices, std::vector<xla::int64> strides)
      : Node(ir::OpKind(at::aten::xla_slice),
             {input}, [&]() {
       xla::XlaBuilder b("InferOutputShape");
       auto input_ir = xla::Parameter(&b, 0, input.shape(), "p0");
       xla::XlaOp result = xla::Slice(
         input_ir, start_indices, limit_indices, strides);
       return XlaHelpers::ShapeOfXlaOp(result);
     },
             /*num_outputs=*/1, xla::util::MHash(start_indices, limit_indices, strides)),
        start_indices_(std::move(start_indices)),
        limit_indices_(std::move(limit_indices)),
        strides_(std::move(strides)) {}

  NodePtr Clone(OpList operands) const override {
    return MakeNode<XlaSlice>(
        operands.at(0), start_indices_, limit_indices_, strides_);
  }

  XlaOpVector Lower(LoweringContext* loctx) const override {
    xla::XlaOp result = xla::Slice(
        loctx->GetOutputOp(operand(0)), start_indices_, limit_indices_, strides_);
    return ReturnOp(result, loctx);
  }

  std::string ToString() const override {
    std::stringstream ss;
    ss << Node::ToString();
    OpFieldToString(ss, "start_indices", start_indices_);
    OpFieldToString(ss, "limit_indices", limit_indices_);
    OpFieldToString(ss, "strides", strides_);
    return ss.str();
  }

 private:
  std::vector<xla::int64> start_indices_;
  std::vector<xla::int64> limit_indices_;
  std::vector<xla::int64> strides_;
};

}  // namespace
}  // namespace ops
}  // namespace ir
}  // namespace swift_xla

OpaqueXLATensor* XLATensor_abs(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Abs>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_acos(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Acos>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_acosh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Acosh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_add(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Add>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_all(OpaqueXLATensor* input, Int64ArrayRef dims, bool keep_reduced_dimensions) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::All>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndices(
          dims.slice(), input_ir_value.shape().rank()),
      keep_reduced_dimensions);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_any(OpaqueXLATensor* input, Int64ArrayRef dims, bool keep_reduced_dimensions) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Any>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndices(
          dims.slice(), input_ir_value.shape().rank()),
      keep_reduced_dimensions);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_argmax(OpaqueXLATensor* input, int64_t dim, bool keepdim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Argmax>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      keepdim);
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Long));
}

OpaqueXLATensor* XLATensor_argmin(OpaqueXLATensor* input, int64_t dim, bool keepdim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Argmin>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      keepdim);
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Long));
}

OpaqueXLATensor* XLATensor_asin(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Asin>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_asinh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Asinh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_atan(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Atan>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_atanh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Atanh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_cat(OpaqueXLATensorArrayRef input, int64_t dim) {
  auto input_ir_value = swift_xla::UnpackIrValues(input);

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Cat>(
      input_ir_value, swift_xla::ir::ops::CanonicalizeCat(input_ir_value, dim));
  return new swift_xla::XLATensor(swift_xla::FirstTensor(input)->CreateFrom(
      swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_ceil(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Ceil>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_clamp(OpaqueXLATensor* t,
                                 OpaqueXLATensor* clipValueMin,
                                 OpaqueXLATensor* clipValueMax) {
  auto t_ir_value = t->GetIrValue();
  auto clipValueMin_ir_value = clipValueMin->GetIrValue();
  auto clipValueMax_ir_value = clipValueMax->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Clamp>(
      t_ir_value, clipValueMin_ir_value, clipValueMax_ir_value);
  return new swift_xla::XLATensor(
      t->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_constant_pad_nd(OpaqueXLATensor* input, Int64ArrayRef pad, XLAScalar value) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::ConstantPadNd>(
      input_ir_value,
      swift_xla::ir::ops::CanonicalizePad(input_ir_value.shape(), pad.slice()),
      atScalar(value));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_cos(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Cos>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_cosh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Cosh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_cumprod(OpaqueXLATensor* input, int64_t dim,
                                   bool exclusive, bool reverse) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Cumprod>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      exclusive, reverse);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_cumsum(OpaqueXLATensor* input, int64_t dim,
                                  bool exclusive, bool reverse) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Cumsum>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      exclusive, reverse);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_diagonal_value(OpaqueXLATensor* input,
                                          int64_t offset, int64_t dim1,
                                          int64_t dim2) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::DiagonalValue>(
      input_ir_value, offset,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim1, input_ir_value.shape().rank()),
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim2, input_ir_value.shape().rank()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_div(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Div>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_dynamic_slice(OpaqueXLATensor* base,
                                         OpaqueXLATensorArrayRef start_indices,
                                         Int64ArrayRef slice_shapes) {
  auto base_ir_value = base->GetIrValue();
  auto start_indices_ir_value = swift_xla::UnpackIrValues(start_indices);

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::DynamicSlice>(
      base_ir_value, start_indices_ir_value,
      swift_xla::XlaHelpers::I64List(slice_shapes.slice()));
  return new swift_xla::XLATensor(
      base->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_dynamic_update_slice(
    OpaqueXLATensor* base, OpaqueXLATensor* update,
    OpaqueXLATensorArrayRef start_indices) {
  auto base_ir_value = base->GetIrValue();
  auto update_ir_value = update->GetIrValue();
  auto start_indices_ir_value = swift_xla::UnpackIrValues(start_indices);

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::DynamicUpdateSlice>(
          base_ir_value, update_ir_value, start_indices_ir_value);
  return new swift_xla::XLATensor(
      base->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_eq(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Eq>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_exp(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Exp>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_expand(OpaqueXLATensor* input, Int64ArrayRef dims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Expand>(
      input_ir_value, swift_xla::ir::ops::CanonicalizeExpand(
                          input_ir_value.shape(), dims.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_expm1(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Expm1>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_flip(OpaqueXLATensor* input, Int64ArrayRef dims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Flip>(
      input_ir_value, swift_xla::ir::ops::CanonicalizeFlip(
                          input_ir_value.shape(), dims.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_floor(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Floor>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_gather(OpaqueXLATensor* input,
                                  OpaqueXLATensor* indices, int64_t start_dim) {
  auto input_ir_value = input->GetIrValue();
  auto indices_ir_value = indices->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Gather>(
      input_ir_value, indices_ir_value, start_dim);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_ge(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Ge>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_gt(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Gt>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_is_finite(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::IsFinite>(input_ir_value);
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_is_inf(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::IsInf>(input_ir_value);
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_is_nan(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::IsNan>(input_ir_value);
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_le(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Le>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_log(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Log>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_log1p(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Log1p>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_log_softmax(OpaqueXLATensor* input, int64_t dim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::LogSoftmax>(
      input_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
                          dim, input_ir_value.shape().rank()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_log_softmax_backward(OpaqueXLATensor* gradOutput,
                                                OpaqueXLATensor* output,
                                                int64_t dim) {
  auto gradOutput_ir_value = gradOutput->GetIrValue();
  auto output_ir_value = output->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::LogSoftmaxBackward>(
          gradOutput_ir_value, output_ir_value,
          swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
              dim, gradOutput_ir_value.shape().rank()));
  return new swift_xla::XLATensor(
      gradOutput->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_logicalAnd(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::LogicalAnd>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_logical_cast(OpaqueXLATensor* input, XLATensorScalarType dtype) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::LogicalCast>(
      input_ir_value, ToScalarType(dtype));
  return new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 0), ToScalarType(dtype)));
}

OpaqueXLATensor* XLATensor_logicalNot(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::LogicalNot>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_logicalOr(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::LogicalOr>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_lt(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Lt>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_matmul(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Matmul>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_max(OpaqueXLATensor* input, int64_t dim,
                               bool keepDim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Max>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      keepDim);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_maximum(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Maximum>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_mean(OpaqueXLATensor* input,
                                Int64ArrayRef reductionIndices, bool keepDims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Mean>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndices(
          reductionIndices.slice(), input_ir_value.shape().rank()),
      keepDims);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_min(OpaqueXLATensor* input, int64_t dim,
                               bool keepDim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Min>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      keepDim);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_minimum(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Minimum>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_mm(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Mm>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_mul(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Mul>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_ne(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Ne>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(lhs->CreateFrom(
      swift_xla::ir::Value(result_node, 0), at::ScalarType::Bool));
}

OpaqueXLATensor* XLATensor_neg(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Neg>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_nll_loss(OpaqueXLATensor* logits,
                                    OpaqueXLATensor* labels,
                                    int64_t ignore_index) {
  auto logits_ir_value = logits->GetIrValue();
  auto labels_ir_value = labels->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::NllLoss>(
      logits_ir_value, labels_ir_value, ignore_index);
  return new swift_xla::XLATensor(
      logits->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_permute_value(OpaqueXLATensor* input,
                                         Int64ArrayRef dims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::PermuteValue>(
      input_ir_value, swift_xla::XlaHelpers::I64List(dims.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_physical_cast(OpaqueXLATensor* input,
                                         XLATensorScalarType dtype) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::PhysicalCast>(
      input_ir_value, ToScalarType(dtype));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_pow(OpaqueXLATensor* input, OpaqueXLATensor* other) {
  auto input_ir_value = input->GetIrValue();
  auto other_ir_value = other->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Pow>(
      input_ir_value, other_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_prod(OpaqueXLATensor* input,
                                Int64ArrayRef reductionIndices, bool keepDims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Prod>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndices(
          reductionIndices.slice(), input_ir_value.shape().rank()),
      keepDims);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor_pair XLATensor_qr(OpaqueXLATensor* input, bool fullMatrices) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Qr>(
      input_ir_value, fullMatrices);
  OpaqueXLATensor_pair result;
  result.x = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
  result.y = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 1)));
  return result;
}

OpaqueXLATensor* XLATensor_relu(OpaqueXLATensor* features) {
  auto features_ir_value = features->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Relu>(features_ir_value);
  return new swift_xla::XLATensor(
      features->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_rem(OpaqueXLATensor* input, OpaqueXLATensor* other) {
  auto input_ir_value = input->GetIrValue();
  auto other_ir_value = other->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Rem>(
      input_ir_value, other_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_repeat(OpaqueXLATensor* input,
                                  Int64ArrayRef multiples) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Repeat>(
      input_ir_value, swift_xla::XlaHelpers::I64List(multiples.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_resize_value(OpaqueXLATensor* input,
                                        Int64ArrayRef dims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::ResizeValue>(
      input_ir_value, swift_xla::XlaHelpers::I64List(dims.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_round_to_even(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::RoundToEven>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_rsqrt(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Rsqrt>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_select(OpaqueXLATensor* input, int64_t dim,
                                  int64_t index) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Select>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      index);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sigmoid(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Sigmoid>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sign(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Sign>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sin(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Sin>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sinh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Sinh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_slice(OpaqueXLATensor* input, int64_t dim, int64_t start, int64_t end, int64_t stride) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Slice>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      start, end, stride);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_softmax(OpaqueXLATensor* input, int64_t dim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Softmax>(
      input_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
                          dim, input_ir_value.shape().rank()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sqrt(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Sqrt>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_squeeze(OpaqueXLATensor* input, int64_t dim) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Squeeze>(
      input_ir_value, swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
                          dim, input_ir_value.shape().rank()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_stack(OpaqueXLATensorArrayRef input, int64_t dim) {
  auto input_ir_value = swift_xla::UnpackIrValues(input);

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Stack>(
      input_ir_value,
      swift_xla::ir::ops::CanonicalizeStack(input_ir_value, dim));
  return new swift_xla::XLATensor(swift_xla::FirstTensor(input)->CreateFrom(
      swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sub(OpaqueXLATensor* lhs, OpaqueXLATensor* rhs) {
  auto lhs_ir_value = lhs->GetIrValue();
  auto rhs_ir_value = rhs->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Sub>(
      lhs_ir_value, rhs_ir_value);
  return new swift_xla::XLATensor(
      lhs->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_sum(OpaqueXLATensor* input,
                               Int64ArrayRef reductionIndices, bool keepDims) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Sum>(
      input_ir_value,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndices(
          reductionIndices.slice(), input_ir_value.shape().rank()),
      keepDims);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor_tuple_3 XLATensor_svd(OpaqueXLATensor* input, bool computeUv,
                                      bool fullMatrices) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Svd>(
      input_ir_value, computeUv, fullMatrices);
  OpaqueXLATensor_tuple_3 result;
  result.v0 = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
  result.v1 = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 1)));
  result.v2 = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 2)));
  return result;
}

OpaqueXLATensor* XLATensor_tan(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Tan>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tanh(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::Tanh>(input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_Conv(OpaqueXLATensor* input,
                                   OpaqueXLATensor* filter, bool depthwise,
                                   Int64ArrayRef strides,
                                   enum TFPadding padding,
                                   Int64ArrayRef explicit_paddings,
                                   enum TFDataFormat data_format,
                                   Int64ArrayRef dilations) {
  auto input_ir_value = input->GetIrValue();
  auto filter_ir_value = filter->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::TfConv>(
      input_ir_value, filter_ir_value, depthwise,
      swift_xla::XlaHelpers::I64List(strides.slice()), ToTFPadding(padding),
      swift_xla::XlaHelpers::I64List(explicit_paddings.slice()),
      x10::ToTFFormat(data_format),
      swift_xla::XlaHelpers::I64List(dilations.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_ConvBackpropFilter(
    OpaqueXLATensor* input, Int64ArrayRef filter_sizes,
    OpaqueXLATensor* out_backprop, bool depthwise, Int64ArrayRef strides,
    enum TFPadding padding, Int64ArrayRef explicit_paddings,
    enum TFDataFormat data_format, Int64ArrayRef dilations) {
  auto input_ir_value = input->GetIrValue();
  auto out_backprop_ir_value = out_backprop->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TfConvBackpropFilter>(
          input_ir_value, swift_xla::XlaHelpers::I64List(filter_sizes.slice()),
          out_backprop_ir_value, depthwise,
          swift_xla::XlaHelpers::I64List(strides.slice()), ToTFPadding(padding),
          swift_xla::XlaHelpers::I64List(explicit_paddings.slice()),
          x10::ToTFFormat(data_format),
          swift_xla::XlaHelpers::I64List(dilations.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_ConvBackpropInput(
    Int64ArrayRef input_sizes, OpaqueXLATensor* filter,
    OpaqueXLATensor* out_backprop, bool depthwise, Int64ArrayRef strides,
    enum TFPadding padding, Int64ArrayRef explicit_paddings,
    enum TFDataFormat data_format, Int64ArrayRef dilations) {
  auto filter_ir_value = filter->GetIrValue();
  auto out_backprop_ir_value = out_backprop->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TfConvBackpropInput>(
          swift_xla::XlaHelpers::I64List(input_sizes.slice()), filter_ir_value,
          out_backprop_ir_value, depthwise,
          swift_xla::XlaHelpers::I64List(strides.slice()), ToTFPadding(padding),
          swift_xla::XlaHelpers::I64List(explicit_paddings.slice()),
          x10::ToTFFormat(data_format),
          swift_xla::XlaHelpers::I64List(dilations.slice()));
  return new swift_xla::XLATensor(
      filter->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_MirrorPad(OpaqueXLATensor* input,
                                        Int64ArrayRef padding,
                                        enum TFMirrorPadMode mode) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::TfMirrorPad>(
      input_ir_value, swift_xla::XlaHelpers::I64List(padding.slice()),
      ToTFMirrorPadMode(mode));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_MirrorPadGrad(OpaqueXLATensor* grad_output,
                                            Int64ArrayRef input_size,
                                            Int64ArrayRef padding,
                                            enum TFMirrorPadMode mode) {
  auto grad_output_ir_value = grad_output->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TfMirrorPadGrad>(
          grad_output_ir_value,
          swift_xla::XlaHelpers::I64List(input_size.slice()),
          swift_xla::XlaHelpers::I64List(padding.slice()),
          ToTFMirrorPadMode(mode));
  return new swift_xla::XLATensor(
      grad_output->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_OneHot(OpaqueXLATensor* indices, OpaqueXLATensor* on_value, OpaqueXLATensor* off_value, int64_t depth, int64_t axis) {
  auto indices_ir_value = indices->GetIrValue();
  auto on_value_ir_value = on_value->GetIrValue();
  auto off_value_ir_value = off_value->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::TfOneHot>(
      indices_ir_value, on_value_ir_value, off_value_ir_value, depth, axis);
  return new swift_xla::XLATensor(
      on_value->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_StatelessRandomUniform(Int64ArrayRef shape, OpaqueXLATensor* seeds, OpaqueXLATensor* minvalue, OpaqueXLATensor* maxvalue) {
  auto seeds_ir_value = seeds->GetIrValue();
  auto minvalue_ir_value = minvalue->GetIrValue();
  auto maxvalue_ir_value = maxvalue->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TfStatelessRandomUniform>(
          swift_xla::MakeArrayShapeFromDimensions(
              shape.slice(), {}, minvalue->shape().get().element_type(),
              minvalue->GetDevice().hw_type),
          seeds_ir_value, minvalue_ir_value, maxvalue_ir_value);
  return new swift_xla::XLATensor(
      minvalue->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_tf_UnsortedSegmentSum(OpaqueXLATensor* data, OpaqueXLATensor* indicies, int64_t num_segments) {
  auto data_ir_value = data->GetIrValue();
  auto indicies_ir_value = indicies->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TfUnsortedSegmentSum>(
          data_ir_value, indicies_ir_value, num_segments);
  return new swift_xla::XLATensor(
      data->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_threshold(OpaqueXLATensor* input, OpaqueXLATensor* output, float threshold, float value) {
  auto input_ir_value = input->GetIrValue();
  auto output_ir_value = output->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Threshold>(
      input_ir_value, output_ir_value, threshold, value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor_pair XLATensor_topk(OpaqueXLATensor* input, int64_t k,
                                    int64_t dim, bool largest) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Topk>(
      input_ir_value, k,
      swift_xla::XlaHelpers::GetCanonicalDimensionIndex(
          dim, input_ir_value.shape().rank()),
      largest);
  OpaqueXLATensor_pair result;
  result.x = new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
  result.y = new swift_xla::XLATensor(input->CreateFrom(
      swift_xla::ir::Value(result_node, 1), at::ScalarType::Long));
  return result;
}

OpaqueXLATensor* XLATensor_truncated_normal(OpaqueXLATensor* input) {
  auto input_ir_value = input->GetIrValue();

  auto result_node =
      swift_xla::ir::MakeNode<swift_xla::ir::ops::TruncatedNormal>(
          input_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_update_slice(OpaqueXLATensor* input,
                                        OpaqueXLATensor* source,
                                        Int64ArrayRef baseIndices) {
  auto input_ir_value = input->GetIrValue();
  auto source_ir_value = source->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::UpdateSlice>(
      input_ir_value, source_ir_value,
      swift_xla::XlaHelpers::I64List(baseIndices.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_where(OpaqueXLATensor* condition, OpaqueXLATensor* input, OpaqueXLATensor* other) {
  auto condition_ir_value = condition->GetIrValue();
  auto input_ir_value = input->GetIrValue();
  auto other_ir_value = other->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::Where>(
      condition_ir_value, input_ir_value, other_ir_value);
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_xla_pad(OpaqueXLATensor* input,
                                   XLAScalar padding_value,
                                   PaddingConfig padding_config) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::XlaPad>(
      input_ir_value, atScalar(padding_value),
      ToXLAPaddingConfig(padding_config));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}

OpaqueXLATensor* XLATensor_xla_slice(OpaqueXLATensor* input, Int64ArrayRef start_indices, Int64ArrayRef limit_indices, Int64ArrayRef strides) {
  auto input_ir_value = input->GetIrValue();

  auto result_node = swift_xla::ir::MakeNode<swift_xla::ir::ops::XlaSlice>(
      input_ir_value, swift_xla::XlaHelpers::I64List(start_indices.slice()),
      swift_xla::XlaHelpers::I64List(limit_indices.slice()),
      swift_xla::XlaHelpers::I64List(strides.slice()));
  return new swift_xla::XLATensor(
      input->CreateFrom(swift_xla::ir::Value(result_node, 0)));
}
